{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf341053-c91e-4d65-962d-2d8b9218f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "class rag_format(BaseModel):\n",
    "    Ranked_Relevant_Information: str = Field(description=\"The ranked pieces of information that will be directly relevant for answering the query.\")\n",
    "    File_Sources: str = Field(description=\"The filenames of the files from which the information was retrieved, with the format '{...}.pdf'.\")\n",
    "\n",
    "class answer_format(BaseModel):\n",
    "    Response: str = Field(description=\"The answer to the question/prompt using the given information only.\")\n",
    "\n",
    "class eval_format(BaseModel):\n",
    "    Evaluation: str = Field(description=\"The evaluation of whether the two answers to the given question are the same or not.\")\n",
    "\n",
    "def rag_eval_agent(question, vector_store, ideal, rag_model) -> str:\n",
    "    rag_message=\"\"\"You are a retrieval agent tasked with performing file searches to find information for the purpose of providing answers.\n",
    "        Find pieces of information that will be directly relevant for answering the query and rank these pieces of information from most relevant to least relevant\n",
    "        You must quote the passages from the files directly. Do not paraphrase or change the text in any way.\n",
    "        Do not include information unless you have a source for that piece of information. \n",
    "        If no information is relevant, you must return a single piece of information, where you state \"No information found\".\n",
    "        Ideally, these pieces of information will be sentences, phrases, data points or sets of data points, but you have limited flexiblility to include other pieces of information if you think they are appropriate.\n",
    "        \n",
    "        You must use tool call (i.e., file search).\n",
    "        \n",
    "        You know about the content of the code-base.\n",
    "        \"\"\"\n",
    "    rag_assistant = client.beta.assistants.create(\n",
    "        name=\"rag_test\",\n",
    "        instructions=rag_message,\n",
    "        tools=[\n",
    "            {\"type\": \"file_search\",\n",
    "                \"file_search\":{\n",
    "                    'max_num_results': 10,\n",
    "                    \"ranking_options\": {\n",
    "                        \"ranker\": \"auto\",\n",
    "                        \"score_threshold\": 0.6\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\":[vector_store.id]}},\n",
    "        model=rag_model, \n",
    "        temperature = 0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": rag_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=question,\n",
    "                    role='user',\n",
    "                )\n",
    "    \n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=rag_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=rag_message,\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status!=\"completed\":\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "    for message in response_messages.data:\n",
    "        for content in message.content:\n",
    "            output=content.text.value\n",
    "            if output.startswith(\"{\"):\n",
    "                data=json.loads(output)\n",
    "                try:\n",
    "                    answer=data[\"Ranked_Relevant_Information\"]\n",
    "                except:\n",
    "                    answer=data[\"Ranked Relevant Information\"]\n",
    "                try:\n",
    "                    sources=data[\"File_Sources\"]\n",
    "                except:\n",
    "                    sources=data[\"File Sources\"]\n",
    "    if not (\"answer\" in locals()):\n",
    "        answer=\"No relevant information.\"\n",
    "    if not (\"sources\" in locals()):\n",
    "        sources=\"No relevant sources.\"\n",
    "    client.beta.assistants.delete(assistant_id=rag_assistant.id)\n",
    "    answer_message=\"\"\"\n",
    "    You are an answering agent tasked with answering a question or providing a summary only using the relevant information or prompts that are given to you, via the \"Ranked Relevant Information\".\n",
    "    Generate a logical and reasoned response to the question or prompts only using the ranked relevant information.\n",
    "    Use the question to provide context to the information before deciding if the information is relevant or not.\n",
    "    If no file sources are given, you must answer \"No information.\".\n",
    "    If you judge pieces of information to be redundant or irrelevant, you may choose to not consider them further in the formulation of your answer, but you must consider the given pieces of information at least once each.\n",
    "    If you do not have any information to answer the question, you must say \"No information given\".\n",
    "    You may say that you do not have enough information to answer the question, if it is appropriate.\n",
    "    \"\"\"\n",
    "    answer_assistant = client.beta.assistants.create(\n",
    "        name=\"answer_test\",\n",
    "        instructions=answer_message,\n",
    "        model=rag_model, \n",
    "        temperature = 0.0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": answer_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=\"Question: \"+question+\"\\nRanked Relevant Information: \"+answer,\n",
    "                    role='user',\n",
    "                )\n",
    "    \n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=answer_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=answer_message,\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status!=\"completed\":\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "    del answer\n",
    "    for message in response_messages.data:\n",
    "        for content in message.content:\n",
    "            output=content.text.value\n",
    "            if output.startswith(\"{\"):\n",
    "                data=json.loads(output)\n",
    "                answer=data[\"Response\"]\n",
    "    if not (\"answer\" in locals()):\n",
    "        answer=\"No information.\"\n",
    "    client.beta.assistants.delete(assistant_id=answer_assistant.id)\n",
    "    eval_message=\"\"\"\n",
    "    You are an evaluation agent tasked with comparing the given two different answers to the same question. \n",
    "    Focus on the meaning of both answers, in the context of the question, when formulating your evaluation.\n",
    "    If a point is conveyed in both answers, as responses to the associated question, output \"Same\".\n",
    "    If a similar points is conveyed in both answers, as responses to the associated question, output \"Similar\".\n",
    "    If all of the points are different in both answers, as responses to the associated question, output \"Different\".\n",
    "    If you are unsure about the above criteria for the answers to the associated question, output \"Unsure\".\n",
    "    Ensure that differences between numerical values and results between the two answers are emphasised in your analysis, unless the question specifically allows for approximations/inexact numerical values. \n",
    "    Then, if the question specifically allows for approximations/inexact numerical values, only compare the numerical values approximately.\n",
    "    \"\"\"\n",
    "    eval_assistant = client.beta.assistants.create(\n",
    "        name=\"eval_test\",\n",
    "        instructions=eval_message,\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature = 0.0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": eval_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=question+answer+str(ideal),\n",
    "                    role='user',\n",
    "                )\n",
    "    \n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=eval_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=eval_message,\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status!=\"completed\":\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "    for message in response_messages.data:\n",
    "        for content in message.content:\n",
    "            output=content.text.value\n",
    "            if output.startswith(\"{\"):\n",
    "                data=json.loads(output)\n",
    "                evaluation=data[\"Evaluation\"]\n",
    "    client.beta.assistants.delete(assistant_id=eval_assistant.id)\n",
    "    ideal_clean= ''.join(char for char in str(ideal) if char.isalnum())\n",
    "    ideal_clean=ideal_clean.upper()\n",
    "    answer_clean=''.join(char for char in answer if char.isalnum())\n",
    "    answer_clean=answer_clean.upper()\n",
    "    if ideal_clean in answer_clean:\n",
    "        simple_eval=\"Simple_Same\"\n",
    "    else:\n",
    "        simple_eval=\"Simple_Different\"\n",
    "    return simple_eval+\" \"+evaluation+\" \"+answer+\" \"+sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b35a8-987f-43b5-9b5c-e153fa04636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lit = pd.read_csv('../cmbagent_dataset/cmbagent_dataset.csv')\n",
    "chunking_strategy =  {\n",
    "        \"type\": \"static\",\n",
    "        \"static\": {\n",
    "            \"max_chunk_size_tokens\": 4000, # reduce size to ensure better context integrity\n",
    "            \"chunk_overlap_tokens\": 100 # increase overlap to maintain context across chunks\n",
    "        }}\n",
    "    \n",
    "vector_store = client.vector_stores.create(name=\"rag_eval_test\", chunking_strategy=chunking_strategy)\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk(assistant_data):\n",
    "    # Filter out unwanted directories like .ipynb_checkpoints\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    for file in files:\n",
    "        if file.startswith('.') or file.endswith('.ipynb')  or file.endswith('.yaml') or file.endswith('.txt') or (not '.' in file):\n",
    "            continue\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "    \n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id,\n",
    "        files=file_streams\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064136ed-b3a2-4746-a8d2-9720c172dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lit = pd.read_csv('../cmbagent_dataset/cmbagent_dataset.csv')\n",
    "vector_store=client.vector_stores.retrieve(vector_store_id=\"vs_67da9f09a6b48191a32189befe73c49e\")\n",
    "vector_store_id=vector_store.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b521f029-d051-4266-933a-f3e807c54ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.047619047619054"
     ]
    }
   ],
   "source": [
    "for i in range(lit.shape[0]):\n",
    "    print(i/lit.shape[0]*100, end=\"\")\n",
    "    print(\"\\r\", end=\"\")\n",
    "    query=lit.loc[i, \"question\"]\n",
    "    output=rag_eval_agent(query, vector_store, lit.loc[i, \"ideal\"], \"gpt-4o-mini\")\n",
    "    with open(\"output_cmbagent1.txt\", \"a\") as file:\n",
    "        file.write(str(output.replace(\"\\n\", \"\"))+\"\\t\"+str(lit.loc[i, \"ideal\"])+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d02e41-e9e4-4508-ad8f-4a50e9040c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = client.beta.vector_stores.files.list(vector_store_id=vector_store.id)\n",
    "for file in files.data:\n",
    "    client.beta.vector_stores.files.delete(vector_store_id=vector_store.id, file_id=file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea6cb96-59e0-4df7-84de-29e958f3c39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: file-4974tk7hacVFHBGtbaAjyQ\n",
      "Deleted file: file-XZcTQabVWJFEnHKk1v8jiw\n",
      "Deleted file: file-UJ192E3Ah5L2hgbrsianP8\n",
      "Deleted file: file-Qsq2nz8zha31iM89kcZtgd\n",
      "Deleted file: file-6sSwjkw7H9dAhYxtb1aDcx\n",
      "Deleted file: file-4AJVFq9gzo5D8AfhDVRnqy\n",
      "Deleted file: file-GszvqFhCBBJRbJ2Hq3KULu\n",
      "Deleted file: file-CAiQxq5rhd25GtNYDAuVJV\n",
      "Deleted file: file-8khMHifL2SRnukSARyuLhb\n",
      "Deleted file: file-EbkenDCB8yoiD2XpsX9Gkt\n",
      "Deleted file: file-5Gc5upjMfCcYJJukY1wYHa\n",
      "Deleted file: file-1CxDCNuRDsg8bJqzwk6Lip\n",
      "Deleted file: file-QtkJBcta6UTtBJrtg5ygjC\n",
      "Deleted file: file-VKm2qZod4zJ29riGzoDNv5\n",
      "Deleted file: file-CwzBGVz1qw4ErK5RFjarHP\n",
      "Deleted file: file-TGFTfDBnkNeqSJe3rPf7yo\n",
      "Deleted file: file-GFFehGKrPFx2JgmNVvKWrc\n",
      "Deleted file: file-P3rcz4WufidH1QEP8ZLtvG\n",
      "Deleted file: file-1tfQRFFc2dxXXquAjoZw2Z\n",
      "Deleted file: file-N8Dhe3tNKvTnN5euLGUFr5\n",
      "Deleted file: file-DVaUt23zKyKf7zKVMADZBw\n",
      "Deleted file: file-LQFLnBxUq3fu5x152UNNj6\n",
      "Deleted file: file-86Ut6KYmDkqs6BUkrK7WDc\n",
      "Deleted file: file-3YmBHpNqsXiD2XWE8SVr9W\n",
      "Deleted file: file-6X5CVxZrFnFqypLda23juR\n",
      "Deleted file: file-9wHK9x97fS1sNsoT4m2xB7\n",
      "Deleted file: file-BKcpj3Ft2wYYaCJh6YxTCC\n",
      "Deleted file: file-JwfAH6XpsCVMBZg1cAwmDS\n",
      "Deleted file: file-7aiiUXsGzjhi1JiV3PYJJ2\n",
      "Deleted file: file-MntgJME8g8wSC83CsuNCGr\n",
      "Deleted file: file-19fTtQrNQL6fwd7u2uzhaa\n",
      "Deleted file: file-7rAXLEwetq2YRtrMQEPE5p\n",
      "Deleted file: file-FcmpuJVh5bYMSLuzfpBVvP\n",
      "Deleted file: file-ALWGy2Bq7Z1GM6opqNT9f9\n",
      "Deleted file: file-96otg16gyKKkGhgca64C4i\n",
      "Deleted file: file-MZJQVjzvPgPfbmDupryVYP\n",
      "Deleted file: file-Y8VfbzMLr4j6XFGhX7fb9o\n",
      "Deleted file: file-U838L8ERLzMPvTAKs46wuc\n",
      "Deleted file: file-3231DA6LVRe9Bo1TUXMpqz\n",
      "Deleted file: file-3y8aKCfS21cqB3xrSGpZYg\n",
      "Deleted file: file-CCdYQdmybRVQHGTDSBqNjH\n",
      "Deleted file: file-78dyUKLf2h9Xs7TuKoBHCp\n",
      "Deleted file: file-ANLU4iyLNgt6tgRoYdswss\n",
      "Deleted file: file-RAogEDQeyoEUxY3PcJCLoC\n",
      "Deleted file: file-39GsT5VJzWYgfdJT3Zsiev\n",
      "Deleted file: file-MKxgwiXsSea6iSAhw2YTMC\n",
      "Deleted file: file-RXxrT8aNo3eqPoPNqgbKnK\n",
      "Deleted file: file-15Lcb95y61t5U3GaB47LKw\n",
      "Deleted file: file-K1bXZvgtmsJvThDc8gUR92\n",
      "Deleted file: file-CJuvXXQfX72u8N89VzJa9o\n",
      "Deleted file: file-YFtmzMCtvN17jHQ1rg4Sud\n",
      "Deleted file: file-NsdFGzppyQLNVdxKTtDajZ\n",
      "Deleted file: file-X4r8mo5n1sMbVaBMNyfBzd\n",
      "Deleted file: file-C9eWFrTdXCBLBsaKC1ZvnM\n",
      "Deleted file: file-UipcdsoJQTLVAiMcSHWCK7\n",
      "Deleted file: file-VwWjutjGD26pLYViHpzsmz\n",
      "Deleted file: file-1vJLsBmFJ3gXwXwtx9z9bj\n",
      "Deleted file: file-BMkTL8A9fd9bZmKfY8EVgY\n",
      "Deleted file: file-79mtR9aNAQyUxWuKb2pHXw\n",
      "Deleted file: file-7RiL23w6vZ6jB7zGevPKtV\n",
      "Deleted file: file-2FjoVeKbz6UQmRtFjHe4bT\n",
      "Deleted file: file-AnuBLPQ3EyrQ3YNzstQbgx\n",
      "Deleted file: file-UJw6HUSMxs63BFWKfhx9TU\n",
      "Deleted file: file-2dyBbGQiPCymgHbpvCERw9\n",
      "Deleted file: file-3PE1TYghQVbpHnZwghLoGa\n",
      "Deleted file: file-JwpxhHrfhS7sNTjyxqU1aW\n",
      "Deleted file: file-178DeCZjuxEwrTofYnc4Rx\n",
      "Deleted file: file-JJBhadMPd2u1JinEsRgT5M\n",
      "Deleted file: file-Qc8tSAUJcDzYrDDsTWANXS\n",
      "Deleted file: file-VwfRUxfkV3Ty1bQs5UXDpa\n",
      "Deleted file: file-1GfmBJp6wntKiijDQzFB43\n",
      "Deleted file: file-RA86CCE5WzxAyEczotSE6d\n",
      "Deleted file: file-J7uJ2FVyj2AbvK5DX9hnt4\n",
      "Deleted file: file-9PqmCTvxi5GK38menX7GhY\n",
      "Deleted file: file-DSU9jBNwczu2mkEHBMz3s5\n",
      "Deleted file: file-NcuAHi9hygvNgtQNyjopjT\n",
      "Deleted file: file-XMPQkv45VUkyYrwBwdGBr6\n",
      "Deleted file: file-95eRzDNu8Q1vnrXQH19wBb\n",
      "Deleted file: file-RMz1dFczHkyhF6P51HmNVp\n",
      "Deleted file: file-QwJyzorji6tVPZygstRkhV\n",
      "Deleted file: file-Qpfmckks5ZJnnJL4JjTMHZ\n",
      "Deleted file: file-5PqCJpdtemiFFDiRgPV3sz\n",
      "Deleted file: file-Sy5WC6JAUeJ6X8JkHhFWKE\n",
      "Deleted file: file-TNNRF8L7V2abwm8KRz9wAb\n",
      "Deleted file: file-KFgFX6hCHT6rFtsGSLr6jB\n",
      "Deleted file: file-MbU7EPAfMk4C9LVKu5Z49p\n",
      "Deleted file: file-NEre4KPGZEZLVsqFpjqJWf\n",
      "Deleted file: file-PSCCeC3zQqW1F87veS3r7r\n",
      "Deleted file: file-XpKqpKEmz4V4xF6u7FpN7Q\n",
      "Deleted file: file-MBwG8JmBaYv6NLFsBdKuuC\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m         client\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mdelete(file_id\u001b[38;5;241m=\u001b[39mfile\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleted file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mdelete(vector_store_id\u001b[38;5;241m=\u001b[39m\u001b[43mvector_store\u001b[49m\u001b[38;5;241m.\u001b[39mid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "files = client.files.list()\n",
    "for file in files.data:\n",
    "    if file.purpose == \"assistants\" and \".pdf\" in file.filename:\n",
    "        client.files.delete(file_id=file.id)\n",
    "        print(f\"Deleted file: {file.id}\")\n",
    "client.beta.vector_stores.delete(vector_store_id=vector_store.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmbagent_env",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
